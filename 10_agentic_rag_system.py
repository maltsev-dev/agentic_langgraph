import os
from typing import Annotated, TypedDict, Sequence   
from langchain_core.messages import BaseMessage, SystemMessage, ToolMessage, HumanMessage
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_classic.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langgraph.prebuilt import ToolNode
from langchain_core.tools import tool
from langgraph.graph.message import add_messages
from langgraph.graph import StateGraph, START, END

from dotenv import load_dotenv
load_dotenv(override=True)


# Temperature is set to 0 to make the agent's responses more deterministic, which is often desirable in a multi-agent system to ensure consistent behavior.
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# Embeddings are used to convert text into vector representations that can be stored in a vector database for efficient retrieval. 
# In this case, we are using the "text-embedding-3-small" model from OpenAI to generate embeddings for the documents that will be stored in the Chroma vector database.
# Embeddind model chould be compatible with the LLM model used in the agent to ensure that the retrieved documents are relevant to the agent's responses.
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

pdf_path = "Contract.pdf"  # Replace with the actual path to your PDF document
if not os.path.exists(pdf_path):
    raise FileNotFoundError(f"The specified PDF document was not found at: {pdf_path}")

pdf_loader = PyPDFLoader(pdf_path)


# check if the PDF document was loaded successfully and handle any potential errors that may occur during the loading process. 
# This is important to ensure that the agent has access to the necessary information from the document for its reasoning and decision-making processes.
try:
    pages = pdf_loader.load()
    print(f"Successfully loaded {len(pages)} pages from the PDF document.")
except Exception as e:
    raise RuntimeError(f"An error occurred while loading the PDF document: {e}")


# Chunking is the process of breaking down large documents into smaller, more manageable pieces (chunks) that can be processed by the agent.
# The RecursiveCharacterTextSplitter is a tool that splits the text into chunks based on character count, 
# ensuring that each chunk does not exceed a specified size (chunk_size) and that there is some overlap (chunk_overlap) between consecutive chunks to maintain context.
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
pages_split = text_splitter.split_documents(pages)

# The Chroma vector database is used to store the vector representations of the document chunks generated by the embeddings model. 
# The persist_directory variable specifies the directory where the vector database will be stored, and the collection_name variable specifies 
# the name of the collection within the database where the document chunks will be stored.
persist_directory = "chroma_db"
if not os.path.exists(persist_directory):
    os.makedirs(persist_directory)
collection_name = "vector_store"


# Actually creating the Chroma vector database and handling any potential errors that may occur during the creation process.
try:
    vector_store = Chroma.from_documents(
        documents=pages_split,
        embedding=embeddings,
        persist_directory=persist_directory,
        collection_name=collection_name
    )
    print(f"Successfully created the Chroma vector database with {len(pages_split)} document chunks.")
except Exception as e:
    raise RuntimeError(f"An error occurred while creating the Chroma vector database: {e}")


# The retriever is a tool that allows the agent to retrieve relevant document chunks from the vector database based on a query.
retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}  # Retrieve the top 5 most similar document chunks. Default is 4
)


# Create tools for the agent to interact with the vector database and retrieve relevant information based on user queries.
@tool
def retrieve_tool(query: str) -> str:
    """Tool for retrieving relevant document chunks based on a query."""
    
    docs = retriever.invoke(query)
    if not docs:
        return "No relevant information in the document found."
    results = []
    for i, doc in enumerate(docs):
        results.append(f"Chunk {i+1}:\n{doc.page_content}\n")
    return "\n\n".join(results)

tools = [retrieve_tool]
llm_with_tools = llm.bind_tools(tools)

# Creating a state graph for the agent, which defines the structure of the agent's memory and how it processes information.
class State(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]


# Create conditional logic for the agent's behavior based on the messages in its memory.
def tool_needed(state: State) -> bool:
    """Determine if the last message contatins tool calls"""
    result = state["messages"][-1]
    return hasattr(result, "tool_calls") and len(result.tool_calls) > 0


system_prompt = SystemMessage(content= f"""You are an Intelligent Agent that can retrieve information from a document using the provided tools. \
    If a query is not related to the document, respond with "I don't know." If you are unsure about an answer, say "I don't know." Do not make up answers. \
    Always use the 'retrieve_tool' to get information from the document when needed.
    please always cite the specific part of the document you are referring to in your answer, for example: "According to Chunk 1: ...""")

# create tool dictionary for easy access to tools by name. 
# This allows the agent to easily call the appropriate tool when needed based on the messages in its memory and the defined conditional logic.
tools_dict = {tool.name: tool for tool in tools}


# LLM Agent function that processes the current state and generates a response based on the messages in its memory and the defined system prompt.
def call_llm(state: State):
    """Call the LLM with the current state."""
    messages = list(state["messages"])
    messages = [system_prompt] + messages
    response = llm_with_tools.invoke(messages)
    return {"messages": [response]}



# Action function that executes the tool calls from the LLM response and updates the state with the tool results.

def take_action(state: State) -> State:
    """ execute tool calls from the LLM response and update the state with the tool results."""
    tool_calls = state["messages"][-1].tool_calls  # Get the tool calls from the last message in the state, which is the response generated by the LLM.  This assumes that the LLM's response includes tool calls that need to be executed.

    results = []
    for t in tool_calls:
        print(f"Calling tool: {t['name']} with query: {t['args'].get('query', 'No query provided')}")
      
        if not t["name"] in tools_dict: # check if the tool name in the tool call is valid and exists in the tools dictionary. If not, raise an error to prevent the agent from attempting to call a non-existent tool, which could lead to unexpected behavior or crashes.
            print(f"\nError: Tool '{t['name']}' not found in tools dictionary.")
            result = "Error: Tool not found. Please check the tool name in the tools dictionary and try again."
    
        else:
            result = tools_dict[t["name"]].invoke(t["args"].get("query", ''))  # Call the tool with the provided arguments and get the result
            print(f"Result length: {len(str(result))} ")

        results.append(ToolMessage(tool_call_id=t['id'], name = t['name'], content=str(result)))  # Append the tool result to the results list as a ToolMessage, which will be added to the agent's memory state for future reference and reasoning.
    print(f"Tool execution completed for tool: {t['name']}\n")
    return {"messages": results}


graph = StateGraph(State)
graph.add_node("llm", call_llm)
graph.add_node("action", take_action)

graph.add_conditional_edges("llm", tool_needed,{
    True: "action",
    False: END
})
graph.add_edge("action", "llm")
graph.add_edge(START, "llm")

rag_agent = graph.compile()

# The run_agent function provides a simple command-line interface for interacting with the RAG agent. 
# It continuously prompts the user for input, processes the input through the agent, and prints the agent's response until the user decides to exit.
def run_agent():
    print("\n ===== RAG Agent Execution Started =====\n")

    while True:
        user_input = input("Enter your query (or 'exit' to quit): ")
        if user_input.lower() in ['exit', 'quit']:
            print("\n ===== RAG Agent Execution Ended =====\n")
            break

        messages = [HumanMessage(content=user_input)]

        response = rag_agent.invoke({"messages": messages})

        print("\nANSWER:")
        print(response['messages'][-1].content)

run_agent()